---
title: "Inverse Problem of Diffusion"
author: "Martin Outzen Berild"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: pdf_document
---
```{r loadlibs, echo = F, include=F}
library(tidyverse)
```
We consider an inverse problem with a basis in the following differential equation 
\begin{equation}
\frac{du(x,t)}{dt} = \frac{d^2u(x,t)}{dx^2}, \enspace u(x,0) = h_0(x), \enspace x \in (0,1), t \ge 0.
\end{equation}
Data is $u(x,t) = h_t(x)$ for a given time $t>0$. The aim of the inverse problem is $h_0(x)$.

The forward model can be written as 
\begin{equation}
u(x,t) = h_t(x) = \frac{1}{\sqrt{4\pi t}}\int e^{-(x-y)^2/(4t)}h_0(y)dy, \enspace t\ge0.
\end{equation}
Using discretization we get

\begin{equation}
 \bf{h}_t = \left[
  \begin{array}{c}
    h_t(x_1)\\
    h_t{x_2}\\
    \vdots \\ 
    h_t{x_N}
  \end{array}
 \right] = A \left[
 \begin{array}{c}
    h_0{x_1}\\
    h_0{x_2}\\
    \vdots\\
    h_0{x_N}
 \end{array}\right]
 = A\bf{h}_0 ,
\end{equation}
where a regular grid of N = 100 points is used, such that $x_1 = 0$, $x_2 = 0.01$, ..., $x_N = 0.99$. The sequence $x$ is created in **R** by the the code
```{r createX}
x = seq(from = 0, to = 0.99, by = 0.01)
```
The interval (0,1) is made into a circle, i.e. 1 corresponds to 0. The matrix A has elements
\begin{equation}
A(i,j) = \frac{0.01}{\sqrt{4\pi t}}e^{-|x_i - x_j|^2/(4t)}.
\end{equation}
The distance $|x_i-x_j|$ is modular on the circle (0,1).
The *createA* function in below calculates the matrix $A$ for a given position $x$ and time $t$
```{r createA}
createA <- function(x,t){
  A = matrix(NA, nrow = length(x), ncol = length(x))
  for (i in seq(length(x))){
    for (j in seq(length(x))){
      d = min(abs(x[i]-x[j]),1-abs(x[i]-x[j]))
      A[i,j] = 0.01/sqrt(4*pi*t)*exp(-d^2/(4*t))
    }
  }
  A
}
```

Measurements $\bf{y} = (y_1,...,y_N)'$ are acquired at time t = 0.001 (1ms): 
\begin{equation}
y_i = h_t(x_i) + \epsilon_i, \enspace \epsilon_i \sim \mathcal{N}(0, 0.025^2), \enspace \mathrm{iid}.
\end{equation}

The observations $y$ are downloaded, imported into **R** and converted to vector form. 
```{r read_data}
y = read.delim2(file = "OppgA.txt", header = F, sep = "\n", dec = ".")[[1]]
```

The observations are presented in Figure \ref{fig:datafig}

```{r datafig, echo = F, fig.width=4,fig.height=4,fig.cap="\\label{fig:datafig}Observations  $(y_1,...y_{100})'$ that are informative of the latent process $h_t(x)$ at time $t=1$ms."}
df = data.frame(x = x, y = y)
ggplot(df, aes(x = x, y = y)) + 
  geom_point() + 
  theme_classic()
```


# Exercise a

We want to solve the inverse problem directly by $A^{-1}\bf{y}$. First we compute the eigenvalues of the matrix. The observations $y$ are collected at time $t = 1$ms, and we firstly initialize the matrix A.

```{r initA}
A = createA(x,t = 0.001)
image(A[,nrow(A):1])
```

The eigenvalues of $A$ can easily be calculated in **R** and are shown in Figure \ref{fig:eigfig}.
```{r calc_eigen}
S = eigen(A)[[1]]
```

```{r eigAfig, echo = F, fig.width=4,fig.height=4,fig.cap="\\label{fig:eigfig}Eigenvalues of $A$ at time $t = 1$ms"}
eig.df <- data.frame(i = seq(length(S)), values = S)
ggplot(eig.df, aes(x = i, y = values)) +
  geom_point() + 
  theme_classic()
```


The singular value decomposition can be found by finding the eigenvectors of $A^TA$ and $AA^T$. Then since our matrix $A$ is square we can use its eigenvalues in the formula 
\begin{equation}
A = USV^T, 
\end{equation}
where $U$ contains the eigenvectors of $AA^T$, $V$ the eigenvectors of $A^TA$ and $S$ the eigenvalues of $A$. 

```{r svd}
U = eigen(A%*%t(A))[[2]]
V = eigen(t(A)%*%A)[[2]]
```

We want to approximate this solution using a filter. The approximation is given by
\begin{equation}
\bf{\hat{h}}_0 = \sum\limits_{\{i:\sigma_i>0\}} \phi_i(\alpha) \frac{<u_i,y>}{\sigma_i}v_i,
\end{equation}
where $\phi_i(\alpha)$ is the filter applied. In our case we want to truncate the small eigenvalue of $A$, and this is done by the truncated singular value expansion which uses the filter $\phi_i(\alpha) = I\{\sigma_i>\alpha\}$. The choice of $\alpha$ which yields the best solution is not known however. 

```{r tsvd}
tsvd <- function(alpha,y,U,S,V){
  res = numeric(length(S))
  for (i in seq(length(S))){
    if(S[i]>alpha){
      res = res + (((U[,i]%*%y)[[1]])/S[i])*V[,i]
    }
  }
  res
}

h0 <- tsvd(alpha = 0.1,y,U,S,V)
```


```{r tsvdfig, echo = F, fig.width=4,fig.height=4,fig.cap="\\label{fig:h0fig1}$\bf{h}_0(x_i) visualized against the $x_i$ values."}
tsvd.df <- data.frame(x = x, h0 = h0)
ggplot(tsvd.df, aes(x = x, y = h0)) + 
  geom_point() + 
  theme_classic()
```


# Exercise b

Assume that we now add prior information to $\bf{h}_0$ in the form of a Gaussian prior, $\bf{h}_0 \sim \mathcal{N}(0,\bf{I})$. We want to find the posterior expectation $E(\bf{h}_0|\bf{y})$. We also have knowledge that the error $\bf{\epsilon} \sim \mathcal{N}(0,0.25^2\bf{I})$. We have the linear relationship $\bf{y} = A\bf{h}_0 + \bf{\epsilon}$ and we have the posterior distribution given by
\begin{equation*}
p(\bf{h}_0|\bf{y}) = \frac{p(\bf{y},\bf{h}_0)}{p(\bf{y})} = \frac{p(\bf{y}|\bf{h}_0)p(\bf{h}_0)}{p(\bf{y})}.
\end{equation*}
It is hard to anytically find the posterior mean, but if we represent the gaussian random function $H_0$ and $\epsilon$ by the Karhunen-Loève expansion given by
\begin{equation*}
\begin{array}{rcl}
H_0 &=& \sum\limits_{i=1}^\infty H_{0,i}\bf{v}_i \\
\epsilon &=& \sum\limits_{i=1}^\infty \epsilon_i \bf{v}_i , 
\end{array}
\end{equation*}
this will make the estimation easier. 
This makes $\{H_0\}_{i=1}^\infty$ be independent Gaussian random variable with mean $\mu_i$ and variance $\gamma_i^2$, and $\{\epsilon_i\}_{i=1}^\infty$ becomes  gaussian random variable with variance $\lambda_i^2$. The matrix $A$ still is represented by the singular system $\{\sigma_i^2,v_i,u_i\}_{i=1}^\infty$, with $\sigma_i$, $u_i$ and $v_i$ being from $S$, $U$ and $V$ respectivly. The posterior random function can also be represented by this Karhunen-Loève expansion given by 
\begin{equation*}
(H_0|Y=y) = \sum\limits_{i = 1}^\infty (H_{0,i}|Y_i = y_i)\bf{v}_i,
\end{equation*}
with the set $\{(H_{0,i}|Y_i = y_i)\}_{i=1}^\infty$ being independent Gaussian random variables with expected value $y_i\sigma_i/(\sigma_i^2 + \lambda_i^2/\gamma_i^2)$ and variance $\gamma_i^2[1-\sigma_i^2/(\sigma_i^2+\lambda_i^2/\gamma_i^2)]$. This yields the joint posterior expectation
\begin{equation*}
E(H_0|Y=y) = \sum\limits_{i=1}^{100} \frac{y_i\sigma_i}{\sigma_i^2 + \frac{\lambda_i^2}{\gamma_i^2}}.
\end{equation*}

```{r test}
Q_e = 4*diag(length(y))
Q_h0 = diag(length(y))
test123<- function(y,Q_e,A,Q_h0){
  mu_h0 = numeric(length(y))
  eta = (t(y)%*%Q_e)%*%A + mu_h0%*%Q_h0
  Sigma = (t(A)%*%Q_e)%*%A + Q_h0
  var_post = solve(Sigma)
  e_post = Sigma%*%t(eta)
  return(list(e_post = e_post,var_post=var_post))
}
res = test123(y,Q_e,A,Q_h0)
post.h0 = data.frame(i = seq(length(y)),expect = res$e_post, stdev = sqrt(diag(res$var_post)))
ggplot(post.h0,aes(x=i)) + 
  geom_point(aes(y = expect), color = "deepskyblue") + 
  geom_errorbar(aes(ymin = expect - stdev, ymax = expect + stdev))
```

```{r test2}
createQh0 <- function(x){
  Q_h0 = matrix(NA, nrow = length(x), ncol = length(x))
  for (i in seq(length(x))){
    for (j in seq(length(y))){
      d = min(abs(x[i]-x[j]),1-abs(x[i]-x[j]))
      Q_h0[i,j] = exp(-d/0.1)
    }
  }
  solve(Q_h0)
}
Q_h0 = createQh0(x)
res2 = test123(y,Q_e,A,Q_h0)
post2 = data.frame(i = seq(length(y)), 
                   y= y, 
                   expect = res2$e_post, 
                   lb = res2$e_post - 1.96*sqrt(diag(res2$var_post)), 
                   ub = res2$e_post + 1.96*sqrt(diag(res2$var_post)))
ggplot(post2, aes(x = i)) + 
  geom_point(aes(y = y),color = "deepskyblue") +
  geom_line(aes(y = expect), color = "firebrick") + 
  geom_ribbon(aes(ymin = lb, ymax = ub),alpha = 0.3)
```

```{r postexp }
post.expect <- function(lambda,gamma,y,S){
  res = numeric(length(y))
  for (i in seq(1,length(y))){
    res[i] = (y[i]*S[i])/(S[i]^2 + lambda^2/gamma^2)
  }
  res
}
# lambda = epsilon, gamma = h_0
e_post <- post.expect(lambda = 0.25, gamma = 1, y, S)
```


Since we have independent random variables they are uncorrelated and therby yielding the join posterior variance 
\begin{equation*}
\mathrm{Var}(\sum\limits_{i=1}^{100} (H_{0,i}|Y_i=y_i)) = \sum\limits_{i=1}^{100} \mathrm{Var}(H_{0,i}|Y_i = y_i) = \sum\limits_{i = 1}^{100} \gamma_i^2\left[1 - \frac{\sigma_i^2}{\sigma_i^2 + \frac{\lambda_i^2}{\gamma_i^2}}\right]
\end{equation*}.

```{r postvar}
post.var <- function(lambda,gamma,y,S){
  res = numeric(length(y))
  for (i in seq(1,length(y))){
    res[i] = gamma^2*(1-S[i]^2/(S[i]^2 + lambda^2/gamma^2))
  }
  res
}
var_post <- post.var(lambda = 0.25, gamma = 1, y, S)
```
The optimal estimator is the represented by the following equation
\begin{equation*}
\hat{\bf{h}}_0 = \sum\limits_{i=1}^{100} \frac{\sigma_i^2}{\sigma_i^2 + \frac{\lambda_i^2}{\gamma_i^2}}\frac{<\bf{u}_i,\bf{y}>}{\sigma_i}\bf{v}_i
\end{equation*}

```{r baysol}
bay_sol <- function(lambda,gamma,y,U,S,V){
  res = numeric(length(y))
  for (i in seq(length(y))){
    res = res + (S[i]^2/(S[i]^2 + lambda^2/gamma^2)*((U[,1]%*%y)[[1]])/S[1])*V[,1]
  }
  res
}
h0 <- bay_sol(lambda = 0.25, gamma = 1, y, U, S, V)
```

```{r figbaysol, echo = F}
bay.df <- data.frame(x = x, h0 = h0, e_post = e_post, sd_post = sqrt(var_post))
ggplot(bay.df,aes(x = x)) + 
  geom_point(aes(y = h0), color = "deepskyblue") + 
  geom_errorbar(aes(ymin = e_post - sd_post, ymax = e_post + sd_post))
```

# Exercise c

We will next assume that the prior information of $\bf{h}_0$ is in the form of the Gaussian prior, $\bf{h}_0 \sim \mathcal{N}(0,\Sigma)$. Where the covariance matrix is given by $\Sigma(i,j) = \exp(-|x_i - x_j|/0.1$. 

```{r creatSig}
createSig <- function(x){
  res = diag(1, nrow = length(x), ncol = length(x))
  for (i in seq(length(x))){
    for (j in seq(i,length(x))){
      res[i,j] = exp(-abs(x[i]- x[j])/0.1)
      res[j,i] = res[i,j]
    }
  }
  res
}
sig = createSig(x)
```


```{r baysol2}


```

